{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# initialize tokenizer and model from pretrained GPT2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Generative pre-trained transformers are a family of language models generally trained on a large corpus of text data to generate human-like text. They are built using several blocks of the transformer architecture. They can be fine-tuned for various natural language processing tasks such as text generation, language translation, and text classification.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8645,   876,   662,    12, 35311,  6121,   364,   389,   257,  1641,\n",
       "           286,  3303,  4981,  4143,  8776,   319,   257,  1588, 35789,   286,\n",
       "          2420,  1366,   284,  7716,  1692,    12,  2339,  2420,    13,  1119,\n",
       "           389,  3170,  1262,  1811,  7021,   286,   262, 47385, 10959,    13,\n",
       "          1119,   460,   307,  3734,    12, 28286,   276,   329,  2972,  3288,\n",
       "          3303,  7587,  8861,   884,   355,  2420,  5270,    11,  3303, 11059,\n",
       "            11,   290,  2420, 17923,    13]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(sentence, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 8645,   876,   662,    12, 35311,  6121,   364,   389,   257,  1641,\n",
       "           286,  3303,  4981,  4143,  8776,   319,   257,  1588, 35789,   286,\n",
       "          2420,  1366,   284,  7716,  1692,    12,  2339,  2420,    13,  1119,\n",
       "           389,  3170,  1262,  1811,  7021,   286,   262, 47385, 10959,    13,\n",
       "          1119,   460,   307,  3734,    12, 28286,   276,   329,  2972,  3288,\n",
       "          3303,  7587,  8861,   884,   355,  2420,  5270,    11,  3303, 11059,\n",
       "            11,   290,  2420, 17923,    13,   198,   198,  7003,   517,  3716,\n",
       "           621, 10224,   662,    12, 35311,  6121,   364,    11,   777,  2512,\n",
       "           286,  6121,   364,   481,  1249,   329, 12846,   290,  6942, 11059,\n",
       "           286,  3288,  3303,  7587,  4133,   884,   355,  2420,   284,   517,\n",
       "          3716,  5479,   319,   257,  4025,  5046,    13,  1119,   423,   867,\n",
       "         13391,   290,   867, 38457,    13,  1119,   743,   307,  3614,   416,\n",
       "           511,  2176,  2854,  4661,    11,   543,   389,  4047,  7885,    13,\n",
       "           554,   749,  2663,    11,  2158,    11,   511, 23989,   318,   262,\n",
       "          1994, 13213,  5766,   290,   326,   460,  1577, 13391,   625,  3218,\n",
       "           662,    12, 35311,  6121,   364,   611,   262, 23392,  2512,   318,\n",
       "          5625,   284,  4047,  3716,  2761,   884,   355,  2939,  7587,    13,\n",
       "           198,   198,   464,  3381,   366, 14681,   290, 15772,  3288,  3303,\n",
       "             1,   468,   587,   973,   287,   867,  8950,   326, 37773,   290,\n",
       "         15772,  3303,    13,  2102,    11,   340,   468,  1690,   587,  5625]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we pass a maximum output length of 200 tokens\n",
    "outputs = model.generate(inputs, max_length=200, do_sample=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generative pre-trained transformers are a family of language models generally trained on a large corpus of text data to generate human-like text. They are built using several blocks of the transformer architecture. They can be fine-tuned for various natural language processing tasks such as text generation, language translation, and text classification.\\n\\nAlthough more complex than conventional pre-trained transformers, these block of transformers will allow for flexible and efficient translation of natural language processing resources such as text to more complex applications on a larger scale. They have many advantages and many disadvantages. They may be limited by their specific performance goals, which are highly variable. In most cases, however, their optimization is the key determining factor and that can give advantages over regular pre-trained transformers if the optimized block is applied to highly complex problems such as image processing.\\n\\nThe term \"process and translate natural language\" has been used in many languages that encode and translate language. However, it has often been applied'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoding\n",
    "# Our generate step outputs an array of tokens rather than words. \n",
    "# To convert these tokens into words, we need to .decode them.\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generative pre-trained transformers are a family of language models generally trained on a large corpus of text data to generate human-like text. They are built using several blocks of the transformer architecture. They can be fine-tuned for various natural language processing tasks such as text generation, language translation, and text classification.\\n\\nAlthough more complex than conventional pre-trained transformers, these block of transformers will allow for flexible and efficient translation of natural language processing resources such as text to more complex applications on a larger scale. They have many advantages and many disadvantages. They may be limited by their specific performance goals, which are highly variable. In most cases, however, their optimization is the key determining factor and that can give advantages over regular pre-trained transformers if the optimized block is applied to highly complex problems such as image processing.\\n\\nThe term \"process and translate natural language\" has been used in many languages that encode and translate language. However, it has often been applied'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True, temperature=1, top_k=50)\n",
    "# We can add more randomness with temperature — the default value is 1, \n",
    "# a high value like 5 will produce a pretty nonsensical output:\n",
    "\n",
    "\n",
    "# add the top_k parameter — which limits the sample tokens to a given number of the most probable tokens. \n",
    "# This results in text that tends to stick to the same topic (or set of words) for a longer period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
