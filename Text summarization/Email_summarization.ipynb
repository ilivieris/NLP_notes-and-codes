{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module for E-mail Summarization\n",
    "*****************************************************************************\n",
    "Input Parameters:\n",
    "    emails: A list of strings containing the emails\n",
    "Returns:\n",
    "    summary: A list of strings containing the summaries.\n",
    "*****************************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from talon.signature.bruteforce import extract_signature\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(email):\n",
    "    \"\"\"\n",
    "    Performs preprocessing operations such as:\n",
    "        1. Removing signature lines (only English emails are supported)\n",
    "        2. Removing new line characters.\n",
    "    \"\"\"\n",
    "    email, _ = extract_signature(email)\n",
    "    \n",
    "    lines = email.split('\\n')\n",
    "    for j in reversed(range(len(lines))):\n",
    "        lines[j] = lines[j].strip()\n",
    "        if lines[j] == '':\n",
    "            lines.pop(j)\n",
    "    \n",
    "    return ' '.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(email):\n",
    "    \"\"\"\n",
    "    Splits the emails into individual sentences\n",
    "    \"\"\"        \n",
    "    \n",
    "    sentences = sent_tokenize(email)\n",
    "    for j in reversed(range(len(sentences))):\n",
    "        sent = sentences[j]\n",
    "        sentences[j] = sent.strip()\n",
    "        if sent == '':\n",
    "            sentences.pop(j)\n",
    "        \n",
    "    return (sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoding(email, max_length=128):\n",
    "    \"\"\"\n",
    "    Obtains sentence embeddings for each sentence in the emails\n",
    "    \"\"\"\n",
    "    from transformers import BertTokenizer, BertForMaskedLM\n",
    "    import torch\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    inputs = tokenizer(email, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')   \n",
    "\n",
    "        \n",
    "    return inputs['input_ids'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(processed_email_text:list = None, encoded_email_text:np.ndarray = None):\n",
    "    '''\n",
    "        Email summarization\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        processed_email_text: list with sentences composing the email \n",
    "        encoded_email_text:\n",
    "    '''\n",
    "\n",
    "    n_clusters = int(np.ceil(len(encoded_email_text)**0.5))\n",
    "    print('Number of clusters: ', n_clusters)\n",
    "\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans = kmeans.fit(encoded_email_text)\n",
    "    print('Kmeans trained')\n",
    "\n",
    "    avg = []\n",
    "    closest = []\n",
    "    for j in range(n_clusters):\n",
    "        idx = np.where(kmeans.labels_ == j)[0]\n",
    "        avg.append(np.mean(idx))\n",
    "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, encoded_email_text)\n",
    "    ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "    \n",
    "    return ' '.join([processed_email_text[closest[idx]] for idx in ordering])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: cover_letter.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Email_examples/cover_letter.txt') as f:\n",
    "    email_text = f.read()\n",
    "print(email_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email pre-processing...\n",
      "Splitting into sentences...\n",
      "Number of sentences:  3\n",
      "Encoding process...\n",
      "Number of clusters:  2\n",
      "Kmeans trained\n",
      "Summary:\n",
      " Good evening Mrs. Yoo, I'm reaching out on behalf of LettuceEat to thank you for your review of our restaurant on ReviewIt. We really appreciate your kind words and recommending our restaurant to others on the platform.\n"
     ]
    }
   ],
   "source": [
    "print('Email pre-processing...')\n",
    "processed_email_text = preprocess(email_text)\n",
    "\n",
    "print('Splitting into sentences...')\n",
    "processed_email_text = split_sentences(processed_email_text)\n",
    "print('Number of sentences: ', len(processed_email_text))\n",
    "\n",
    "print('Encoding process...')\n",
    "encoded_email_text = Encoding(processed_email_text)    \n",
    "\n",
    "summary = summarization(processed_email_text, encoded_email_text)\n",
    "\n",
    "print('\\nSummary:\\n', summary)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: appreciating_the_customer.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good evening Mrs. Yoo,\n",
      "\n",
      "I'm reaching out on behalf of LettuceEat to thank you for your review of our restaurant on ReviewIt. \n",
      "We really appreciate your kind words and recommending our restaurant to others on the platform. \n",
      "\n",
      "LettuceEat is so happy you enjoyed our vegan options and your experience with us. \n",
      "\n",
      "\n",
      "Please come back soon!\n",
      "\n",
      "Best regards,\n",
      "Sarah Gibbs\n"
     ]
    }
   ],
   "source": [
    "with open('Email_examples/appreciating_the_customer.txt') as f:\n",
    "    email_text = f.read()\n",
    "print(email_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email pre-processing...\n",
      "Splitting into sentences...\n",
      "Number of sentences:  3\n",
      "Encoding process...\n",
      "Number of clusters:  2\n",
      "Kmeans trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Good evening Mrs. Yoo, I'm reaching out on behalf of LettuceEat to thank you for your review of our restaurant on ReviewIt. We really appreciate your kind words and recommending our restaurant to others on the platform.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Email pre-processing...')\n",
    "processed_email_text = preprocess(email_text)\n",
    "\n",
    "print('Splitting into sentences...')\n",
    "processed_email_text = split_sentences(processed_email_text)\n",
    "print('Number of sentences: ', len(processed_email_text))\n",
    "\n",
    "print('Encoding process...')\n",
    "encoded_email_text = Encoding(processed_email_text)    \n",
    "\n",
    "summary = summarization(processed_email_text, encoded_email_text)\n",
    "\n",
    "print('\\nSummary:\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: cover_letter.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Email_examples/cover_letter.txt') as f:\n",
    "    email_text = f.read()\n",
    "print(email_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Email pre-processing...')\n",
    "processed_email_text = preprocess(email_text)\n",
    "\n",
    "print('Splitting into sentences...')\n",
    "processed_email_text = split_sentences(processed_email_text)\n",
    "print('Number of sentences: ', len(processed_email_text))\n",
    "\n",
    "print('Encoding process...')\n",
    "encoded_email_text = Encoding(processed_email_text)    \n",
    "\n",
    "summary = summarization(processed_email_text, encoded_email_text)\n",
    "\n",
    "print('\\nSummary:\\n', summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24513103fde56bb2b83e620cc549278f11e79fde4f670db3dc95f99c43b58a75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
